# MLAPP 读书笔记 - 01 概论

> A Chinese Notes of MLAPP，MLAPP 中文笔记项目 
https://zhuanlan.zhihu.com/python-kivy

翻译：[cycleuser](https://www.zhihu.com/people/cycleuser/activities)
2018年05月06日14:04:48

## 1.1 机器学习：概念和目的

> “我们被信息淹没，对知识感到饥饿。” John Naisbitt

人类进入大数据时代了。（译者注：现在有一种乱象，就是什么都说大数据啊人工智能啊，这些头猪已经就跟云计算一样四处飞啊，懂和不懂的都跟着吹捧。）

这么多数据就需要自动化方法来进行数据分析，因此需要机器学习。机器学习也就可以定义成一系列能够自动检测数据模式的方法集合，这些方法可以将发现的模式用于对未来数据的预测，或者对其他具有不确定性过程的决的选取提供参考（例如如何收集更多的数据等）。

本书的观点是解决这些问题的最佳方法就是使用概率论这一工具。概率论可以被用于任何涉及到不确定性的问题。在机器学习里，不确定性来源很多，这一点前言里面都说了，这里重复了，就不赘述了。

本赎回介绍一系列概率模型，适合不同的数据和用途。此外还会介绍一系列的学习算法来使用这些模型。本书目标不仅仅是提供个菜谱式的参考，而是就着不同领域的用途，通过概率模型来给出一个独特角度。

需要注意的是，有时候虽然你有个看上去特别大规模的数据集，但是有效的数据点的个数可能并没那么大，甚至还可能很小。实际上大范围的数据就有个众所周知的特点叫长尾（Long Tail），就是说有少数样本是非常常用的，而大多数是很少见的情景。例如，每天有 20% 的谷歌搜索都是之前没被看到过的。本书中所讨论的核心统计问题是从相对比较小规模的样本来进行泛化，在大数据领域也还是很相关的。



### 1.1.1 机器学习的类别

机器学习一般分成两种主要类型。

第一种就是预测类或者说监督学习，目标是通过学习算法得到一个从输入特征 x 到输出 y 的映射关系（mapping），给定一个带有标签的输入输出集合对 $D = \{(x_i,y_i)\}^N_{i=1}$。这样的集合 D 就叫训练集，N 就是训练集的样本数。
简单来说，每个训练样本的输入特征 $x_i$ 是一个数值为值的 D 维向量，可能就是一个人的身高体重等等。这样的指标叫做特征（features）、属性（attributes）、或者 协变量（covariates）。不过这个 $x_i$ 也可以是一个复杂结构对象，比如一个突破、一句话、一封邮件、一个时间序列、一个分子形状、一个图结构等等。

与此相似，输出（output）或称之为 响应变量（response variable）原则上也可以是任意形式的。不过大多数方法都是假设 $y_i$ 是某个优先集合中的分类变量或者名义变量。比如可以使性别为雌性雄性，可以使实数值的标量（比如收入水平）。当 y 是分类变量的时候，问题就是一个分类问题或者模式识别问题，如果 y 是实数值的变量，则称为回归问题。还有一种变体，叫做有序回归，这是当 y 是某种自然排序标准的时候，比如从 A 到 F 分级。

第二种就是描述性或者叫无监督学习。这里的给定训练集中只有输入值， $D = \{(x_i)\}^N_{i=1}$，目标是要从数据中找到感兴趣的模式。这种方法也被叫做知识发现（knowledge discovery）。这类问题就没有上一种那么详细的定义，因为不知道要找的是啥样子的哪种类型的模式，也没有什么明显的误差矩阵之类的能用。而上一种监督学习里面我们可以把预测值和真实值进行对比。

实际上还有第三种机器学习，叫做强化学习（reinforcement learning），这个当时用的还不那么多。这种方法用于给定奖励或者惩罚信号下的行为响应。例如小孩学走路等等。强化学习超出了本书的讨论范围，不过相关的理论在本书的 5.7也有涉及，但仅仅是基础内容。

## 1.2 监督学习

先开始讨论的是监督学习，这个在实践中用的最广泛。

### 1.2.1 分类问题

分类问题的核心目标就是对输入特征 x 和输出值 y 之间建立映射。其中的 y 属于一个有限集合，这个有限集合的元素个数为C。如果 C =2，那么这个问题就称之为二分类问题，也就可以假设 y 属于集合{0,1}；如果 C > 2，那么久成为多类别分类。如果分类标签不是互斥的，比如一个人可以即被形容为 瘦，同时也被形容为 高，那么称这类问题为多标签分类问题。不过最好把这种问题看做是对多个相关的二元类别标签进行预测的问题，所以也可以称之为多输出模型。除非特地说明，本文说的分类都值得是多类别分类，有单一的输出。

可以用函数估计来将正规表达这种问题。假设 y=f(x) 中的 x 和 y 分别是输入和输出值，而 f 是未知函数，那么目标就是通过给定的做好了分类的训练集来估计这个函数 f，然后使用估计得到的 f 来对新的数据集进行预测。预测的时候要用新的输入特征，也就是之前没看到的，这个过程也叫泛化，根据新特征的函数值就作出预测了。



#### 1.2.1.1 分类问题样本

本节简单又无聊，略


#### 1.2.1.2 概率预测

要处理不确定的情况，就要用到概率了。如果对概率论的基本概念不熟悉了，可以等着本书第二章那里有回顾。
给定输入向量 x 和训练集 D，我们将在可能的分类标签上的概率分布表示为 p(y|x,D)。还记得 y 所属集合的元素个数 C 么？一般来说刚刚这个概率分布表示了一个长度为 C 的向量。如果只有两个分类，自然是所有分类的概率加一起等于1， p(y=1|x,D)+p(y=0|x,D) =1，那么只用其中一个概率 p(y=1|x,D)就足够了。这里用 p(y|x,D)来表示，|右边有 x 和 D，意思是这个概率是在测试样本中输入值 x 和训练集 D 上的条件概率分布。当选用不同的模型的时候，还可以加一个 M 在|右边，表示所选模型，也就是 p(y|x,D,M)。如果行文背景中已经对模型有充分讲述了，就可以把 M 去掉，只写作 p(y|x,D)了。

给定某个可能的概率化的输出，就可以计算出这个猜测是真实标签的概率了：

$\hat y =\hat f (x)=argmax ^C_{c=1} p(y=c|x,D)$ (1.1)

上面这个形式对应的就是最可能的分类标签，也被叫做概率分布 p(y|x,D)的模（mode），也成为最大后验估计（MAP estimate， maximum a posteriori）。更正规详细的介绍在本书的 5.7。


如果一个 p($\hat y$|x,D) 远小于1，也就是说这个答案不让人很有信心，这时候与其返回一个不太可信的结果就不如直接返回不确定了。在医疗和金融等对风险敏感的领域尤其如此。这一部分还列举了 IBM 的 Watson 和 Google 的 SmartASS 系统等等，在此忽略。



#### 1.2.1.3 现实世界中的应用

分类可能是机器学习最广泛的用途了，用于很多有意思又难以人力解决的现实问题。

##### 文档分类和垃圾邮件过滤

![](https://github.com/Kivy-CN/MLAPP-CN/blob/master/Figure/1.2.png?raw=true)


文档分类的目标是对一个文档，比如网页或者电子邮件信息，分到 C 种类别当中的某一种，也就是要计算 p(y=c|x,D)，其中的输入特征 x 是文本的某种信息。
垃圾邮件过滤是这种用途的一个特例，其中分类只有两种，要么是垃圾邮件，y=1，要么不是，则 y=0.

大多数分类器都假设输入特征向量 x 有固定尺寸。可变长度文档的特征向量格式可以使用词汇袋（bag of words）来表示。细节可以参考本书的 3.4.4.1，不过基本思想还是恩简单的，就是如果单词 j 出现在了文档 i 里面，则 $x_{ij}=1$。如果把这种变换应用到数据集中的所有文档，就得到了一个由文档和词汇组成的二元共生矩阵，入图1.2所示。


这样就从一个文档分类问题简化到一个寻找模式中的细微变化的问题了。比如，可能很多邮件含有一些特定关键词，可以借助这些来分类。在练习8.1和8.2里面，你就要自己动手来使用不同分类技巧来进行垃圾邮件识别。



##### 鸢尾花分类

![](https://github.com/Kivy-CN/MLAPP-CN/blob/master/Figure/1.3.png?raw=true)

图1.3是另外一个例子，来自统计学家 Ronald Fisher。这个案例的目的是让机器学习对是三种鸢尾花进行分类，分别是 setosa、versicolor 和 virginica。很幸运，不用直接处理图像，有生物学家已经将有用特征进行了统计，这些特征包括：萼片长度、宽度，花瓣长度、宽度。
从照片等复杂对象到数据这个过程叫做特征提取（feature extraction），这个过程非常重要，又特别困难。译者注：特征提取还往往可能被人忽略，如果没有进行充分的特征提取，是根本不可能充分对分类对象进行分类的。本书后面的章节会讲到从数据中提取好的特征的一些方法。

![](https://github.com/Kivy-CN/MLAPP-CN/blob/master/Figure/1.4.png?raw=true)

如图1.4所示，对鸢尾花数据集进行散点图投图，很明显可以检查花瓣长宽来讲 setosas（红色圆圈）和其他两种区分开。然而另外两种 versicolor 和 virginica 的区分就稍微难一点了，需要至少使用两组特征。在进行机器学习之前，可以将数据先投图看看，这样可以进行一些探索性的数据分析，是个很好的办法。


##### 图片分类和手写识别

![](https://github.com/Kivy-CN/MLAPP-CN/blob/master/Figure/1.5.png?raw=true)

接下来这个问题就更难了，要直接对图片进行分类，这些图片都是没有预处理的数据。可能需要先整体分类一下，比如是室内的还是室外场景，是横着还是竖着拍摄的，是否有小狗等等，这就叫做图形分类。

有一个特例就是判断手写的字母数字，例如对于邮编之类的，就可以进行这种手写识别。这个用途也有个标准数据集，叫做 MNIST，是 Modified National Institute of Standards 的缩写，这个数据集里面的图片都做了预处理，保证了大多数数值或者字幕都在图片中心位置。这个数据集有6000个训练样本图片和 10000 个测试样本图片，内容都是由不同的人写的数字0到9。每个图片都是 28*28 像素大小，灰度值都是从0到255的。图1.5（a）是一些样本。

很多通用分类方法会忽略输入特征的结构，例如空间布局等等。因此，这些方法可以用于处理图1.5（b） 所示的数据，这份数据是同一份数据对所有特征进行了随机排列。这个过程在联系1.1当中。这种灵活性既是好事，也是噩梦，是好事因为这些方法可以适用于通用目的，是噩梦因为这些方法忽略了很明显的有用信息。本书后面会讨论利用输入特征结构信息的方法。


##### 人脸检测和识别

![](https://github.com/Kivy-CN/MLAPP-CN/blob/master/Figure/1.6.png?raw=true)

这个问题就比上一个更难了，要从一个图片中找到某些对象，这也叫做对象检测或者对象定位。一个例子就是人脸检测。解决这个问题的一种方法是把图片切分成叠覆在不同位置、大小和方向的小块，然后对这些歌小块来检测是否含有人脸形状的结构。这种方法也叫做滑动窗口检测（sliding windows detector）。这个系统会返回像人脸概率最高的区域的位置。如图1.6所示。这种方法目前应用于数码相机里面，可以用于控制相机去对焦到人脸位置。译者注：比如索尼微单就有人脸识别和眼控对焦，都是基于这种思路。另外一种用途是在谷歌街景之类的系统中把人脸模糊掉。

找到了面孔之后，就可以继续进行人脸识别了，就是对该面孔所有者的身份进行识别。这个过程中用到的分类标签可能很多很多。另外这个过程用到的这些特征都和人脸检测问题中有所不同，例如在人脸识别的过程中，面孔的细微差别，例如发行等等，对于身份确定都是很重要的；而对于人脸检测来说，这类细节都是无关紧要的，而关注的核心是人脸与非人脸之间的差别。




### 1.2.2 回归

回归和分类其实挺相似的，核心区别在于分类的响应变量 y，也就是分类标签，是离散的，是一个有限集合中的元素；而回归中的响应变量 y 是连续的。
如图1.7所示，有一个单实数值的输入特征 $x_i$∈ R，然后也有一个单实数值的响应变量 $y_i$∈ R。我们可以考虑对这个样本数据使用两种模型进行拟合，一个如图1.7左图所示，用直线，另外一个如右图所示，用二次曲线拟合。其实这时候就能引出很多扩展问题了，比如如果输入特征是高维度的怎么办、异常值怎么处理、非光滑的响应变量怎么办等等。这些内容都在本书的后文中。

![](https://github.com/Kivy-CN/MLAPP-CN/blob/master/Figure/1.7.png?raw=true)

现实世界中的回归问题有很多，比如：
* 根据给定的市场状况和其他方面的信息预测明天的股票市场价格
* 预测在 YouTube 上观看某个特定视频的观众年龄
* 预测一个机器人手臂在三维空间中的位置，对其一系列不同的马达发送控制信号控制扭矩
* 根据不同的临床检测结果来预测人体中前列腺特异抗原（prostate specific antigen, PSA）的规模



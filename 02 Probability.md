




# MLAPP 读书笔记 - 02 概率

> A Chinese Notes of MLAPP，MLAPP 中文笔记项目 
https://zhuanlan.zhihu.com/python-kivy

记笔记的人：[cycleuser](https://www.zhihu.com/people/cycleuser/activities)

2018年05月07日11:07:07

## 2.1 概率论简介


>概率论就是把常识降维表达成计算而已。---皮埃尔 拉普拉斯（Pierre Laplace）1812

前面的章节里，可以看出概率论在机器学习里面扮演了很重要的角色。所以本章就详细讲一下概率论。不过本章内容不可能面面俱到而且也不会过分强调细节，所以你最好还是找一本参考书来看看啥的。本章会涉及到后文要用到的很多关键概念和思路。

在讲解具体内容之前，先停下来想一下，什么是概率？我们都很熟悉这种说法：一枚硬币人头朝上的概率是0.5.但这句话到底是什么意思？实际上对概率有两种不同解读。

第一种是频率论阐述（frequentist interpretation），这种观点认为概率表现力时间长期法师的频率。例如上一句中所说，只要足够多次地抛硬币，人头朝上的概率就是一半。

另一种是对概率的贝叶斯阐述（Bayesian interpretation）。这种观点是概率是用来衡量某种事物的不确定性（uncertainty），与信息更相关，而不是试验的重复（Jaynes 2003）。按照这种观点，上面说硬币的那句话的意思就是我们相信下次抛硬币两面朝上的概率各半。

贝叶斯解释的阐述的一个很大的好处是可以用于对具有不确定性的时间进行建模，而不必要进行长期频率测试。比如估算到 2020年的时候极地冰盖的融化量。这事情可能发生也可能不发生，但不能重复啊。我们也本应对某事的不确定性进行定量衡量；基于我们队这件事发生概率的认知，就可以采取近似行动，这部分参考本书5.7 讲解了在不确定情况下最优选择的相关讲解。在机器学习方面一个例子就是电子邮件判断是否为垃圾邮件。再比如就是雷达上发现一个小点，我们可能要计算该位置对应物体的概率分布，推断是鸟、飞机还是导弹。所有这类场景中，重复试验的思路都是不可行的，但贝叶斯阐述则是可用的，而且也符合直觉。因此本书对概率的解读就采用了贝叶斯阐述。好在概率论的基础内容都是相似的，也不受到所选阐述方式的影响。

> 此处查看原书中图2.1



## 2.2 概率论简单回顾

这部分就是简单回顾一下概率论的基础内容，读者如果对概率论生疏了可以看看，如果还很熟悉这些基本内容 就没必要看了，略过即可。


### 2.2.1 离散随机变量

表达式p(A)是指 A 事件发生（为真）的概率。A 可以是逻辑判断表达式，比如：“明天会下雨”。根据概率定义就可以知道 $0\leq p(A) \leq 1$，如果p(A)=0则意味着绝对不会发生，如果p(A)=1则意味着必然发生。用$p(\bar A)$表示事件 A 不发生的概率；很显然，$p(\bar A) = 1- p(A)$。当事件 A 为真的时候通常还简写做  A=1，反之写为 A=0。这就跟布尔运算那个类似了。

对这个二值事件的记号进行扩展，可以定义一个离散随机变量（discrete random variable）X，这个随机变量可以从任意的一个有限元素集合或者无限但可列的集合 *X* 中取值。将 Xx 的概率记作$p(X=x)$，或者缩写成$p(x)$。这里的$p()$也叫做概率质量函数（probability mass function，缩写为 pmf）。跟上面的一样，也要满足 $0\leq p(x) \leq 1$ 和$\sum_{x\in X}p(x)$。如图2.1所示的就是两个密度函数，定义在有限状态空间$x=\{1,2,3,4,5\}$。做梦的是一个正态分布，$p(x)=1/5$，右面的则是一个退化分布（degenerate distribution），$p(x)=\prod (x=1)$，其中的$\prod()$是二值指标函数（binary indicator function）。这个分布表示的是 X 就总是1，也就是说是固定值。


### 2.2.2 基本规则

这部分讲的是概率论基本规则。

#### 2.2.2.1 两个事件的结合概率

给定两个事件，A 和 B，可以用如下方式来定义结合概率：

$p(A ∨ B) = p(A) + p(B) − p(A ∧ B)$ （2.1）
$p(A ∨ B) = p(A) + p(B)$ 若两个事件互斥（mutually exclusive）（2.2）

译者注：其实2.2毫无必要，因为两个事件互斥的时候也就是2.1里面的最后一项$p(A ∧ B) =0 $所以根本没必要单独定义。 




#### 2.2.2.2 联合概率

两个事件 A 和 B 的联合概率定义如下所示：

$p(A,B) = p(A ∧ B) = p(A|B)p(B)$ （2.3）

这也叫做乘法规则（product rule）。对两个事件的联合概率分布$p(A,B)$，可以以如下方式定义边缘分布(marginal distribution)：

$p(A)=\sum_b p(A,B) =\sum_b p(A|B=b)p(B=b)$(2.4)

上式中对所有可能的状态 B 来进行求和。对 $p(B)$的定义与之相似。也有时候也叫做加法规则（sum rule）或者全概率规则（rule of total probability）

乘法规则可以多次使用，就得到了链式规则（chain rule）：

$p(X_{1:D})=p(X_1)p(X_2|X_1)p(X_3|X_2,X_1)p(X_4|X_1,X_2,X_3) ... p(X_D|X_{1:D-1})$（2.5）

上面的1:D表示的是有序集合{1,2,...,D}。


#### 2.2.2.3 条件概率

若事件 B 为真，在此条件下事件 A 的条件概率如下所示：

$p(A|B) = p(A,B)/p(B)  if p(B)>0 $ （2.6）

### 2.2.3 贝叶斯规则

结合条件概率的定义以及乘法规则和加法规则，就能推出贝叶斯规则（Bayes rule），也称为贝叶斯定理（Bayes Theorem）：


$p(X=x|Y=y) = \frac{p(X=x,Y=y) }{p(Y=y) } = \frac{p(X=x)p(Y=y|X=x)}{\sum_{\dot x}p(X=\dot x)p(Y=y|X=\dot x) }$ （2.7）


#### 2.2.3.1 样例：医疗诊断

假如一位四十岁的女性，决定通过乳腺 X光检测（mammogram）做乳腺癌检测。如果结果是阳性，那么有多大概率患上？很明显这依赖于检测本身的可靠性。假设这个检测的敏感度（sensitivity）是80%，也就是如果一个人患上了，那么被检测出来是阳性的概率为0.8.即：

$p(x=1|y=1)=0.8$(2.8)

其中的 x=1意思就是检测结果阳性，而 y=1的意思是确实患病。
据此很多人就认为患病概率也就是80%。这是错误的！
这个计算方法忽略了患病的先验概率（prior probability），即：
$p(y=1)=0/004$(2.9)

把这个先验概率忽略掉，就是基本比率谬误（base rate fallacy）。此外还要考虑的就是测试本身的假阳性（false positive）或者假警报（false alarm）的概率：

$p(x=1|y=0)$(2.10)

上面三个项目都结合起来，应用贝叶斯规则，可以计算正确概率了：

$\begin{aligned}
p(y=1|x=1) & =\frac{p(x=1|y=1)p(y=1)}{p(x=1|y=1)p(y=1)+p(x=1|y=0)p(y=0)}\\
& =\frac{0.8 \times 0.004}{0.8 \times 0.004 +0.1 \times 0.996}\\
& =0.031   
\end{aligned}
$(2.11、2.12)


上式中$p(y=0)=1-p(y=1)=0.996$。也就是说即便检测结果是阳性，患病概率也只有3%而已。

#### 2.2.3.2 样例：生成分类器

对上面医疗诊断的例子进行泛化，就可以对任意类型的特征向量 x 来进行分类了，如下所示：

$p(y=c|x,\theta)=\frac{p(y=c|\theta)p(x|y=c,\theta)}{\sum_{\dot c}p(y=\dot c|\theta)p(x|y=\dot  c,\theta)}$(2.13)

这就是生成分类器（Generative classiﬁers），使用类条件概率密度$p(x|y=c)$和类先验概率密度$p(y=c$来确定如何生成数据。更多细节在本书第3、4章。另一种方法是直接拟合类后验概率密度$p(y=c|x)$，这就叫辨别式分类器（discriminative classifier）。这两种方法的优劣在本书8.6有详细讲解。



### 2.2.4 独立分布和有条件独立分布

> 此处查看原书中图2.2

X 和 Y 为无条件独立（unconditional independent）或者边缘独立（marginally independent），记作 $X \bot Y$，如果用两个边缘的积来表示，则如图2.2所示：

$X \bot Y \iff p(X,Y)=p(X)p(Y) $(2.14)

总的来说，如果联合分布可以写成边缘的积的形式，就可以说一系列变量之间相互独立（mutually independent）。

不过很可惜，这种无条件独立的情况是很少见的，大多数情况下变量之间都互相关联。好在一般这种影响都可以通过其他变量来传导的，而不是直接的关联。如果X 和 Y 对于给定的 Z 来说有条件独立，则意味着能够将条件联合分布写成条件边缘的积的形式，就说 ：

$X \bot Y |Z \iff p(X,Y|Z)=p(X|Z)p(Y|Z) $(2.15)

在本书第10章会谈到图模型，到时候会把这个假设写成图 X-Z-Y 的形式，更直观表现了 X 和 Y 之间的独立性由 Z 传导。例如，确定当天是否下雨（事件 Z），则明天是否下雨（事件 X）和当天土地是否湿润（事件 Y）之间独立。
直观来说，这是因为 Z 可以同时导致 X 和 Y，所以如果知道了 Z，就不需要知道 Y 就可以预测 X，反之亦然。第10章会详细介绍这部分内容。

条件独立分布的另外一个特点是：

#### 定理 2.2.1 
$X\bot Y| Z $则意味着存在着函数 g 和 h ，对全部的x,y,z，在p(z)>0的情况下满足下列关系：

$p(x,y|z)=g(x,z)h(y,z)$(2.16)

练习2.8有详细证明过程。

条件概率假设让我们可以从小处着手构建大兴的概率模型。本书中有很多这样的样例，尤其是本书的3.5当中就提到了朴素贝叶斯分类器（naive Bayes classifiers），在本书17.2还有马尔科夫模型（Markov models），在本书第10章有图模型（graphical models），所有这些模型都充分利用了条件概率的性质。


### 2.2.5 连续随机变量

签名谈到的都是具有不确定性的离散量。接下来就要将概率论扩展应用到具有不确定性的连续量。

设 X 是一个不确定的连续量。X 在某个空间$a\leq X\leq b$的概率可以用如下方式计算。
先定义如下事件：
$$
\begin{aligned}
A & =(X\le a)\\
B & =(X\le B)\\
W & =(a\le X\le b)
\end{aligned}
$$

很明显有
$B=A\bigvee W$，由于 A 和 W 是相互独立的，所以可以用加法规则：
$p(B)=p(A)+p(W)$(2.17)
显然有：
$p(w)=p(B)-p(A)$(2.18)

> 此处查看原书中图2.3


定义一个函数$F(q) *= p(X\le q)$，这就是 X 的累积密度函数（cumulative distribution function，缩写为 cdf）。很明显这个 cdf 是一个单调递增函数（monotonically increasing function）。如图2.3（a）所示。来利用这个记号则有：

$p(a<X\le b) =F(b)-F(a)$(2.19)

接下来假设这个函数 F(x)可导，则定义函数$f(x)=\frac{d}{dx}F(x)$，这个函数就叫概率密度函数（probability density function，缩写为 pdf）。参考图2.3（b）。有了 pdf，就可以计算一个有限区间上的连续变量的概率了：

$P(a<X\le b)= \int_a^b f(x)dx$(2.20)


随着这个区间逐渐缩小，直到趋向于无穷小，就可以得到下面的形式：

$P(x<X\le x+dx)\approx  p(x)dx$(2.21)

我们要满足$p(x)\ge 0$，但对于任意的 x，$p(x)\ge 1$也有可能，只要密度函数最终积分应该等于1就行了。举个例子，下面的正态分布Unif(a,b）：

$Unif(x|a,b)= \frac{1}{b-a}\prod(a\le x\le b)$(2.22)

如果设置 $a=0,b=\frac1 2$，则有了对于在$x\in [0,\frac 12]$之间取值的任意 x 都有 $p(x)=2$，


### 2.2.6 分位数

由于累积密度函数（cdf） F 是一个单调递增函数，那就有个反函数，记作$F^-$。如果 F 是 X 的累积密度函数（cdf），那么$F^{-1}(\alpha)$就是满足概率$P(X\le x_\alpha )=\alpha $的值；这也叫做 F 的 $\alpha$分位数（quantile）。$F^{-1}(0.5)$就是整个分布的中位数（median），左右两侧的概率各自一半。而$F^{-1}(0.25)$和$F^{-1}(0.75)$则是另外两个分位数。

利用这个累积密度函数（cdf）的反函数还可以计算尾部概率（tail area probability）。例如，如果有高斯分布 $N(0,1)$，$\phi$ 是这个高斯分布的累积密度函数（cdf），这样在$\phi^{-1}(\alpha/2)$ 左边的点就包含了$\alpha /2$概率密度，如图2.3（b）所示。与之对称，在$\phi^{-1}(1-\alpha/2)$ 右边的点也包含了$\alpha /2$概率密度。所以呢，在$(\phi^{-1}(\alpha/2),\phi^{-1}(1-\alpha/2))$ 就包含了$1-\alpha$的概率密度。如果设置$\alpha =0.05$，那么中间这个区间就占了全部概率的95%了。

$(\phi^{-1}(0.025),\phi^{-1}(0.975))=(-1.96，1.96)$（2.23） 

如果这个正态分布是$N(\mu,\sigma^2)$，那么其95%的区间就位于$(\mu-1.96\sigma，\mu+1.96\sigma)$。有时候就简写一下成了$\mu\pm 2\sigma$。


### 2.2.7 均值（Mean）和方差（variance）

对正态分布来说，大家最常见常用的性质估计就是均值（mean），或者称之为期望值（expected value），记作$\mu$。对于离散 rv （译者注：这个 rv 很突兀，之前没出现，也没解释是啥）的情况，可以定义成$E[X] *= \sum_{x\in X}xp(x)$；对于连续 rv 的情况，可以定义为$\E[X] *= \int_{X}xp(x)dx$。如果这个积分是无穷的，则均值不能定义，更多例子后文会有。

然后就是方差（variance）了，这个表征的是分布的“散开程度（spread）”，记作$\sigma^2$。定义如下：

$$
\begin{aligned}
var[X] * & =E[(X-\mu)^2]=\int(x-\mu)^2p(x)dx      &\text{           (2.24)}\\
& =  \int x^2p(x)dx  +\mu^2 \int p(x)dx-2\mu\int xp(x)dx=E[X^2]-\mu^2         &    \text{           (2.25)}\\
\end{aligned}
$$

从上面的式子就可以推导出：
$E[X^2]= \mu^2+\sigma^2 $(2.26)
然后就可以定义标准差（standard deviation）了：
$std[X]*= \sqrt {var[X]}$(2.27)
标准差和 X 单位一样哈。


## 2.3 常见的离散分布 

本节介绍的是一些常用的定义在离散状态空间的参数分布，都是有限或者无限可列的。

### 2.3.1 二项分布和伯努利分布

设咱们抛硬币 n 次，设$X\in \{0,...,n\}$ 是人头朝上的次数。如果头朝上的概率是$\theta$，这就可以说 X 是一个二项分布（binomial distribution），记作$X∼Bin(n,\theta)$。则 pmf（概率质量函数）可以写作：
$Bin(k|n,\theta)*= \binom{n}{k} \theta ^k  (1- \theta)^{n-k}$（2.28）
上式中的
$ \binom{n}{k} *= \frac{n!}{(n-k)!k!}$(2.29)
是组合数，相当于国内早期教材里面的$C_n^k$，从 n 中取 k 个样的组合数，也是二项式系数（binomial coefficient）。如图2.4所示就是一些二项分布。

> 此处查看原书中图2.4



这个分布的均值和方差如下所示：
$mean=\theta, var =n\theta(1-\theta)$（2.30）

换个思路，如果只抛硬币一次，那么$X\in \{0,1 \}$就是一个二值化的随机变量，成功或者人头朝上的概率就是$\theta$。这时候就说 X 服从伯努利分布（Bernoulli distribution），记作$X∼Ber(\theta)$，其中的 pmf 定义为：
 
$Ber(x|\theta)=\theta^{\prod (x=1)}(1-\theta)^{\prod (x=0)}$(2.31)

也可以写成：
$$
Ber(x|\theta)=\begin{cases} \theta &\text{ if x =1} \\
1-\theta &\text{ if x =0} \end{cases}
$$ (2.32)

很明显，伯努利分布只是二项分布中 n=1 的特例。


### 2.3.2 多项式（multinomial）分布和多重伯努利（multinoulli）分布

二项分布可以用于抛硬币这种情况的建模。要对有 K 个可能结果的事件进行建模，就要用到多项分布（multinomial distribution）。这个定义如下：设$x=(x_1,...,x_K)$ 是一个随即向量，其中的$x_j$是第 j 面出现的次数个数。这样 x 的概率质量函数 pmf 就如下所示：

$Mu(x|n,\theta)*- \binom {n}{x_1,..,x_K}\prod^K_{j=1}\theta^{x_j}_j$(2.33)

其中的$\theta_j$是第 j 面出现的概率，另外那个组合数的计算如下所示：
$\binom {n}{x_1,...,x_K} *= \frac{n!}{x_1!x_2!...x_K!}$(2.34)

这样得到的也就是多项式系数（multinomial coefficient），将一个规模为$n=\sum^K_{k=1}$的集合划分成规模从$x_1$到$x_K$个子集的方案数。 

接下来设 n=1。这就好比是讲一个 K 面的骰子只投掷一次，所以 x 就是由 0 和 1 组成的向量。其中只有一个元素会是1.具体来说就是如果 k 面朝上，就说第 k 位为1。这样就可以把 x 看做一个用标量分类的有 K 个状态的随机变量，这样 x 就是自己的虚拟编码（dummy encoding），即：$x=[\prod(x=1),...,\prod(x=K)]$。例如，如果 K=3，那么状态1、2、3对应的虚拟编码分别是(1,0,0),(0,1,0),(0,0,1)。这样的编码也称作单位有效编码（one-hot encoding），因为只有一个位置是1.这样对应的概率质量函数 pmf 就是：

$Mu(x|1,\theta)=\prod^K_{j=1 }\theta_j ^{\prod(x_j=1)}$(2.35)

可以参考图2.1的（b-c）作为一个例子。这是类别分布（categorical distribution）或者离散分布（discrete distribution）的典型案例。Gustavo Lacerda 建议大家称之为多重伯努利分布（multinoulli distribution），这样与二项分布/伯努利分布的区别关系相仿。本书就采用了这个术语，使用下列记号表示这种情况：

$Cat(x|\theta)*= Mu(x|1,\theta)$(2.36)

 换句话说，如果$x∼Cat(\theta)$，则$p(x=j|theta)=\theta_j$。参考表2.1。




#### 2.3.2.1 应用：DNA 序列模体

> 此处查看原书中图2.5


生物序列分析（biosequence analysis）是一个典型应用案例，设有一系列对齐的 DNA 序列，如图2.5（a）所示，其中有10行（序列），15列（沿着基因组的位置）。如图可见其中几个位置是进化的保留位，是基因编码区域的位置，所以对应的列都是“纯的”，例如第7列就都是 G。

如图2.5（b）所示，对这种数据的可视化方法是使用序列标识图（sequence logo）。具体方法是把字幕ACGT 投到对应位置上，字体大小与其实验概率（empirical probability）成正比，最大概率的字幕放在最顶部。

对计数向量归一化，得到在位置$t,\hat \theta_t $的经验概率分布，可以参考本书的公式3.48：
$N_t=(\sum^N_{i=1}\prod(X_{it}=1),\sum^N_{i=1}\prod(X_{it}=2),\sum^N_{i=1}\prod(X_{it}=3),\sum^N_{i=1}\prod(X_{it}=4))$(2.37)
$\hat\theta_t =N_t/N$(2.38)

这个分布就被称作一个模体（motif）。可以计算每个位置上最大概率出现的字母，得到的就是共有序列（consensus sequence）。


### 2.3.3 泊松分布（Poisson Distribution）



如果一个离散随机变量$X\in \{0,1,2,...\}$服从泊松分布，即$X∼ Poi(\lambda)$，其参数$\lambda >0$，其概率质量函数 pmf 为：

$Poi(x|\lambda )=e^{-\lambda}\frac{\lambda ^x}{x!}$(2.39) 

第一项是标准化常数（normalization constant），使用来保证概率密度函数的总和积分到一起是1.
泊松分布经常用于对罕见事件的统计，比如放射性衰变和交通事故等等。参考图2.6是一些投图样本。



### 2.3.4 经验分布（empirical distribution）

某个数据集，$D =\{x_1,...,x_N \}$，就可以定义一个经验分布，也可以叫做经验测度（empirical measure），形式如下所示：

$p_{emp}(A)*=\frac 1 N \sum^N_{i=1}\delta _{x_i}(A)$(2.40)

其中的$\delta_x(A)$是狄拉克测度（Dirac measure），定义为：
$$
\delta_x(A)= \begin{cases} 0 \text{    if }x \notin A \\
1\text{    if }x \in A 
\end{cases}
$$(2.41)

一般来说可以给每个样本关联一个权重（weight）
$p(x)=\sum^N_{i=1}w_i\delta_{x_i}(x)$（2.42）





